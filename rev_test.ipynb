{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58e59990533741a4bcbe746a81258d63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, Phi3ForCausalLM\n",
    "\n",
    "torch.random.manual_seed(0) \n",
    "model = Phi3ForCausalLM.from_pretrained( \n",
    "    \"microsoft/Phi-3-mini-128k-instruct\",  \n",
    "    device_map=\"mps\",  \n",
    "    torch_dtype=\"auto\",  \n",
    "    trust_remote_code=True,  \n",
    ") \n",
    "\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\") \n",
    "\n",
    "# messages = [ \n",
    "#     {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}, \n",
    "#     {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"}, \n",
    "#     {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"}, \n",
    "#     {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"}, \n",
    "# ] \n",
    "\n",
    "# pipe = pipeline( \n",
    "#     \"text-generation\", \n",
    "#     model=model, \n",
    "#     tokenizer=tokenizer, \n",
    "# ) \n",
    "\n",
    "# generation_args = { \n",
    "#     \"max_new_tokens\": 500, \n",
    "#     \"return_full_text\": False, \n",
    "#     \"temperature\": 0.0, \n",
    "#     \"do_sample\": False, \n",
    "# } \n",
    "\n",
    "# output = pipe(messages, **generation_args) \n",
    "# print(output[0]['generated_text']) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi3ForCausalLM(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3Attention(\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "          (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm()\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_attention_layernorm): Phi3RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(input_ids=torch.tensor([[1, 2, 3, 4, 5]], device=\"mps\"), output_hidden_states=True)\n",
    "\n",
    "    sts = output['hidden_states']\n",
    "    print(len(sts))\n",
    "\n",
    "\n",
    "# output['hidden_states']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import math\n",
    "def ff(x):\n",
    "    res = (scipy.special.lambertw(math.exp(-x) * x) + x)\n",
    "    assert res.imag == 0, \"Imaginary part should be 0\"\n",
    "    return res.real\n",
    "# ff(-.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1234, dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "F.silu(torch.tensor(ff(0.12345)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0291, -0.0259,  0.0447,  ...,  0.0099,  0.0425,  0.0603],\n",
       "        [ 0.0195, -0.0459, -0.0908,  ..., -0.0162,  0.0596, -0.0752],\n",
       "        [ 0.1040, -0.0381,  0.0332,  ...,  0.0571,  0.1001, -0.0352],\n",
       "        ...,\n",
       "        [-0.0146, -0.0015, -0.0120,  ...,  0.0063,  0.0135, -0.0063],\n",
       "        [-0.0145, -0.0016, -0.0120,  ...,  0.0064,  0.0135, -0.0064],\n",
       "        [-0.0146, -0.0016, -0.0120,  ...,  0.0063,  0.0134, -0.0064]],\n",
       "       device='mps:0', dtype=torch.bfloat16, requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def solve(A: torch.Tensor, b: torch.Tensor):\n",
    "#     # solve A x = b for x when it is underdetermined. Return the space of all solutions.\n",
    "#     # you can assume that A is full rank.\n",
    "#     # Use SVD to find the null space and particular solution\n",
    "#     U, S, Vh = torch.linalg.svd(A)\n",
    "    \n",
    "#     # Get dimensions\n",
    "#     m, n = A.shape\n",
    "#     rank = m  # Since A is full rank\n",
    "    \n",
    "#     # Get particular solution using pseudoinverse\n",
    "#     S_inv = torch.zeros((n, m), device=A.device, dtype=A.dtype)\n",
    "#     S_inv[:rank, :rank] = torch.diag(1.0 / S[:rank])\n",
    "#     x_particular = Vh.T @ S_inv @ U.T @ b\n",
    "    \n",
    "#     # Get null space basis\n",
    "#     null_space = Vh[rank:].T\n",
    "    \n",
    "#     # Return particular solution and null space basis\n",
    "#     # Any solution can be written as: x = x_particular + null_space @ c \n",
    "#     # where c is any vector of appropriate size\n",
    "#     return x_particular, null_space\n",
    "\n",
    "\n",
    "# rand_inp = torch.randn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_DIM = model.lm_head.weight.shape[0]\n",
    "def sparse_eye(n):\n",
    "    indices = torch.arange(n).unsqueeze(0).repeat(2, 1)\n",
    "    values = torch.ones(n)\n",
    "    return torch.sparse_coo_tensor(indices, values, (n, n))\n",
    "\n",
    "\n",
    "\n",
    "hyperplanes = torch.cat((-torch.ones((N_DIM-1, 1)).to_sparse(), sparse_eye(N_DIM-1)), dim=-1)\n",
    "\n",
    "tst = (hyperplanes.to_dense().to('mps') @ model.lm_head.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polytope as pc\n",
    "\n",
    "pp_top = pc.Polytope(hyperplanes.cpu().to_dense().numpy(), np.zeros(N_DIM-1))\n",
    "\n",
    "pp = pc.Polytope(tst.detach().cpu().numpy(), np.zeros(N_DIM))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reduce(poly, nonEmptyBounded=1, abs_tol=pc.polytope.ABS_TOL):\n",
    "    \"\"\"Remove redundant inequalities from the hyperplane representation.\n",
    "\n",
    "    Uses the algorithm described at [1],\n",
    "    by solving one LP for each facet.\n",
    "\n",
    "    [1] https://www.cs.mcgill.ca/~fukuda/soft/polyfaq/node24.html\n",
    "\n",
    "    Warning:\n",
    "      - nonEmptyBounded == 0 case is not tested much.\n",
    "\n",
    "    @type poly: L{Polytope} or L{Region}\n",
    "\n",
    "    @return: Reduced L{Polytope} or L{Region} object\n",
    "    \"\"\"\n",
    "    if isinstance(poly, pc.Region):\n",
    "        lst = []\n",
    "        for poly2 in poly.list_poly:\n",
    "            red = reduce(poly2)\n",
    "            if pc.is_fulldim(red):\n",
    "                lst.append(red)\n",
    "        if len(lst) > 0:\n",
    "            return pc.Region(lst, poly.props)\n",
    "        else:\n",
    "            return pc.Polytope()\n",
    "    # is `poly` already in minimal representation ?\n",
    "    if poly.minrep:\n",
    "        return poly\n",
    "    # if not is_fulldim(poly):\n",
    "    #     return Polytope()\n",
    "    \n",
    "    print(\"aaa\")\n",
    "    # `poly` isn't flat\n",
    "    A_arr = poly.A\n",
    "    b_arr = poly.b\n",
    "    # Remove rows with b = inf\n",
    "    keep_row = np.nonzero(poly.b != np.inf)\n",
    "    A_arr = A_arr[keep_row]\n",
    "    b_arr = b_arr[keep_row]\n",
    "    neq = np.shape(A_arr)[0]\n",
    "    # first eliminate the linearly dependent rows\n",
    "    # corresponding to the same hyperplane\n",
    "    # Normalize all rows\n",
    "    a_norm = 1 / np.sqrt(np.sum(A_arr.T**2, 0))\n",
    "    a_normed = np.dot(A_arr.T, np.diag(a_norm)).T\n",
    "    remove_row = []\n",
    "    \n",
    "    # Compute all pairwise dot products between normalized vectors\n",
    "    dot_products = np.dot(a_normed, a_normed.T)\n",
    "    \n",
    "    # Get indices of parallel hyperplanes (dot product close to 1)\n",
    "    parallel_pairs = np.where(\n",
    "        np.triu(dot_products > 1 - abs_tol, k=1)\n",
    "    )\n",
    "    \n",
    "    # For each parallel pair, check which inequality constrains more\n",
    "    for i, j in zip(*parallel_pairs):\n",
    "        b_in = b_arr[i] * a_norm[i]\n",
    "        b_jn = b_arr[j] * a_norm[j]\n",
    "        if b_in < b_jn:\n",
    "            remove_row.append(j)\n",
    "        else:\n",
    "            remove_row.append(i)\n",
    "\n",
    "\n",
    "    keep_row = np.setdiff1d(range(neq), remove_row).tolist()\n",
    "    A_arr = A_arr[keep_row]\n",
    "    b_arr = b_arr[keep_row]\n",
    "    neq, nx = A_arr.shape\n",
    "    print(f\"new shape: {A_arr.shape}\")\n",
    "    print(\"bbb\")\n",
    "    if nonEmptyBounded:\n",
    "        if neq <= nx + 1:\n",
    "            return pc.Polytope(A_arr, b_arr)\n",
    "    # # Now eliminate hyperplanes outside the bounding box\n",
    "    # if neq > 3 * nx:\n",
    "    #     lb, ub = pc.Polytope(A_arr, b_arr).bounding_box\n",
    "    #     # Do a coordinate system translation such that the lower bound is\n",
    "    #     # moved to the origin\n",
    "    #     #       A*(x-lb) <= b - A*lb\n",
    "    #     # Relative to the origin, a row ai in A with only positive coefficients\n",
    "    #     # represents an upper bound. If ai*(x1-lb) <= bi,\n",
    "    #     # the hyperplane is above x1.\n",
    "    #     # Hence, if ai*(ub-lb) <= bi, then the hyperplane at row i\n",
    "    #     # does not intersect the bounding box.\n",
    "    #     # The same holds for rows with negative coefficients multiplied with\n",
    "    #     # the origin. Rows with both negative and positive coefficients\n",
    "    #     # are a mixture of the two extremes.\n",
    "    #     cand = ~ (np.dot((A_arr > 0) * A_arr, ub - lb) -\n",
    "    #               (np.array([b_arr]).T - np.dot(A_arr, lb)) < -1e-4)\n",
    "    #     A_arr = A_arr[cand.squeeze()]\n",
    "    #     b_arr = b_arr[cand.squeeze()]\n",
    "    print(\"ccc\")\n",
    "    neq, nx = A_arr.shape\n",
    "    if nonEmptyBounded:\n",
    "        if neq <= nx + 1:\n",
    "            return pc.Polytope(A_arr, b_arr)\n",
    "    # Check for each inequality whether it is implied by\n",
    "    # the other inequalities, i.e., is it redundant?\n",
    "    del keep_row[:]\n",
    "    for k in range(neq):\n",
    "        print(\"ddd \", k)\n",
    "        # Setup object function to maximize the linear function\n",
    "        # defined as current row of A matrix\n",
    "        f = -A_arr[k, :]\n",
    "        G = A_arr\n",
    "        h = b_arr\n",
    "        # Give some slack in the current inequality\n",
    "        h[k] += 0.1\n",
    "        sol = pc.polytope.lpsolve(f, G, h)\n",
    "        h[k] -= 0.1\n",
    "        if sol['status'] == 0:\n",
    "            # If the maximum is greater than the constraint of\n",
    "            # the inequality, then the inequality constrains solutions\n",
    "            # and thus the inequality is non-redundant\n",
    "            obj = -sol['fun'] - h[k]\n",
    "            if obj > abs_tol:\n",
    "                keep_row.append(k)\n",
    "        elif sol['status'] == 3:\n",
    "            keep_row.append(k)\n",
    "    polyOut = pc.Polytope(A_arr[keep_row], b_arr[keep_row])\n",
    "    polyOut.minrep = True\n",
    "    return polyOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaa\n"
     ]
    }
   ],
   "source": [
    "test_res = reduce(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pp.contains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.014  , -0.00577, -0.03903, ..., -0.00753,  0.00492, -0.03903],\n",
       "       [ 0.042  , -0.00385, -0.00362, ...,  0.01491,  0.01819, -0.03013],\n",
       "       [-0.02164,  0.00614,  0.01621, ..., -0.0196 , -0.03234, -0.02635],\n",
       "       ...,\n",
       "       [ 0.00614,  0.01033, -0.02404, ..., -0.0015 , -0.01228, -0.02829],\n",
       "       [ 0.00617,  0.01031, -0.02404, ..., -0.00146, -0.01231, -0.0283 ],\n",
       "       [ 0.00614,  0.0103 , -0.02404, ..., -0.00152, -0.01233, -0.0283 ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_top = torch.cat((torch.ones(1), torch.zeros(N_DIM-1))).cpu().type(torch.DoubleTensor)\n",
    "\n",
    "with torch.no_grad():\n",
    "    sln = torch.linalg.lstsq(model.lm_head.weight.cpu().type(torch.DoubleTensor), one_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp_top.contains(one_top[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.contains(sln.solution[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0720, -0.0677, -0.0846,  ..., -0.0868, -0.0868, -0.0868],\n",
       "       grad_fn=<MvBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst.cpu() @ sln.solution.type(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0, device='mps:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_res = model.lm_head(sln.solution.type(torch.bfloat16).to('mps'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0718, -0.0675, -0.0844,  ..., -0.0866, -0.0866, -0.0866],\n",
       "        grad_fn=<MvBackward0>),\n",
       " tensor(True))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperplanes @ test_res.cpu().type(torch.float32), \\\n",
    "    torch.all(hyperplanes @ test_res.cpu().type(torch.float32) < 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
